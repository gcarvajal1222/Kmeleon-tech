{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import json\n",
    "import textdistance\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import warnings \n",
    "import nltk\n",
    "#from gensim.models import Word2Vec \n",
    "#from gensim.models.wrappers import FastText \n",
    "\n",
    "#from gensim.models import FastText\n",
    "\n",
    "\n",
    "\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import warnings \n",
    "import nltk \n",
    "import pandas as pd\n",
    "from pandas import *\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer() \n",
    "import re\n",
    "from sklearn.feature_extraction import text\n",
    "from builtins import input\n",
    "from statistics import mean\n",
    "from nltk import  pos_tag_sents\n",
    "\n",
    "\n",
    "from statistics import mode, StatisticsError\n",
    "\n",
    "import functools\n",
    "\n",
    "\n",
    "import re\n",
    "from sklearn.feature_extraction import text\n",
    "global cases\n",
    "\n",
    "from pandas import ExcelWriter\n",
    "import xlsxwriter\n",
    "\n",
    "import xlwt\n",
    "from xlwt.Workbook import *\n",
    "from nltk.corpus import wordnet\n",
    "from statistics import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import seaborn as sns\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import sys\n",
    "import difflib\n",
    "import textdistance\n",
    "from pymongo import MongoClient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection(Database(MongoClient(host=['freedevcluster-shard-00-00-dlyph.mongodb.net:27017', 'freedevcluster-shard-00-01-dlyph.mongodb.net:27017', 'freedevcluster-shard-00-02-dlyph.mongodb.net:27017'], document_class=dict, tz_aware=False, connect=True, authsource='admin', replicaset='FreeDevCluster-shard-0', ssl=True), 'chulo-faqs-db'), 'questions')\n",
      "Collection(Database(MongoClient(host=['freedevcluster-shard-00-00-dlyph.mongodb.net:27017', 'freedevcluster-shard-00-01-dlyph.mongodb.net:27017', 'freedevcluster-shard-00-02-dlyph.mongodb.net:27017'], document_class=dict, tz_aware=False, connect=True, authsource='admin', replicaset='FreeDevCluster-shard-0', ssl=True), 'chulo-core-db'), 'baseservices')\n"
     ]
    }
   ],
   "source": [
    "#Connect to static dataframe\n",
    "\n",
    "# df = pd.read_excel(\"faq-extract-text.xlsx\", sheet_name =\"only text\")\n",
    "# ds=pd.Series(df['faq text'].str.lower().unique())\n",
    "# questions_faq = (ds.str.strip())\n",
    "\n",
    "#Connect to Mongo db dev\n",
    "\n",
    "client = MongoClient(\"mongodb+srv://chulo-func-dev:3jbJheEVcHTij73H@freedevcluster-dlyph.mongodb.net/\") \n",
    "\n",
    "# Connect to FAQ db and collection\n",
    "faq_db = client['chulo-faqs-db'] #Get databse\n",
    "questions_col = faq_db['questions'] # Get questions collection\n",
    "print (questions_col)\n",
    "\n",
    "df_faq = DataFrame(list(faq_db.questions.find({})))\n",
    "standarized_Text=pd.Series(df_faq['Text'].str.lower().unique())\n",
    "\n",
    "#Only filter questions that are \"unanswered\"\n",
    "questions_faq = (standarized_Text.str.strip())\n",
    "\n",
    "\n",
    "#Chulo Core db to get services\n",
    "core_db = client['chulo-core-db'] #Get databse\n",
    "base_services = core_db['baseservices']\n",
    "\n",
    "print (base_services)\n",
    "#Connect to list of Base services\n",
    "df_base_services = DataFrame(list(core_db.baseservices.find({})))\n",
    "standarized_base_services=pd.Series(df_base_services['Name'].str.lower().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "standarized_base_services_new = [\"pool\", \"wifi\", \"movie room\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "thresholdSequenceMatcher = 0.80  #Can test with different ratios for different categories \n",
    "def similar(a, b):\n",
    "    return difflib.SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "thresholdJaroWinkler = 0.90  #Can test with different ratios for different categories \n",
    "def similarJaroWinkler(a,_list):\n",
    "    all_scores = []\n",
    "    for i in range(len(_list)):\n",
    "        all_scores.append(textdistance.jaro_winkler.normalized_similarity(a,_list[i]))\n",
    "    return pd.DataFrame({\"similarity scores\":all_scores,\"questions\": _list})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUEST = json.dumps({\n",
    "'body' : {\"text\": \"wheres the pool\"}\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Value of Similar questions\": \"Question Already Exists\", \"Total Ocurrance\": \"2\"}\n"
     ]
    }
   ],
   "source": [
    "# POST /similarity\n",
    "\n",
    "req = json.loads(REQUEST)\n",
    "args = req['body']\n",
    "\n",
    "if 'text' not in args:\n",
    "    print(json.dumps({'text': None}))\n",
    "else:\n",
    "    new_question = pd.Series(str(args[\"text\"])).str.lower().str.strip()\n",
    "    \n",
    "    new_question_strip = [elem.strip().lower().split(' ') for elem in new_question]\n",
    "    flat_new_question = [ item for elem in new_question_strip for item in elem]\n",
    "    \n",
    "    #Find the question in the db if it exists\n",
    "    boolean_findings = questions_faq.str.contains(new_question[0], regex = False)\n",
    "\n",
    "    total_occurence = boolean_findings.sum()# Returns count of all boolean true.\n",
    "    \n",
    "    if(total_occurence > 0):\n",
    "        print(json.dumps({'Value of Similar questions':\"Question Already Exists\", \"Total Ocurrance\": str(total_occurence)}))\n",
    "        \n",
    "\n",
    "#Make comparision if word in Base service exists in the sentence, then only make comparision to those sentence that contains that service\n",
    "\n",
    "def SimilarityComparision():\n",
    "    list_filtered_questions = []\n",
    "    for base_service in standarized_base_services:\n",
    "        if base_service in flat_new_question:\n",
    "            print(base_service)\n",
    "        \n",
    "            for question in questions_faq:\n",
    "                if base_service in question:\n",
    "                    #print(\"true: base service exists in the question\")\n",
    "                    list_filtered_questions.append(question)\n",
    "            \n",
    "            df_similarity_filtered=similarJaroWinkler(new_question[0],list_filtered_questions)\n",
    "            dict_similarity = dict(zip(df_similarity_filtered['questions'], df_similarity_filtered['similarity scores']))\n",
    "            sort_orders={k: v for k, v in sorted(dict_similarity.items(), key=lambda item: item[1], reverse = True)}\n",
    "        #print(json.dumps(sort_orders))\n",
    "            \n",
    "        break;\n",
    "        \n",
    "    else:\n",
    "        df_similarity_filtered=similarJaroWinkler(new_question[0],questions_faq)\n",
    "        dict_similarity = dict(zip(df_similarity_filtered['questions'], df_similarity_filtered['similarity scores']))\n",
    "        sort_orders={k: v for k, v in sorted(dict_similarity.items(), key=lambda item: item[1], reverse = True)}\n",
    "        #print(json.dumps(sort_orders))\n",
    "    \n",
    "    return (json.dumps(sort_orders)) #df_similarity_filtered.sort_values(by=\"similarity scores\",ascending=0).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUEST = json.dumps({\n",
    "'body' : {\"text\": \"where is the pool located\"}\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pool\n",
      "{\"where is the pool\": 0.936, \"where is the pool?\": 0.9248888888888889, \"where's the pool\": 0.8675, \"what time does the pool close?\": 0.8120966183574879, \"when is the pool open\": 0.8105925925925926, \"when does the pool open\": 0.8061642512077295, \"when is the pool open?\": 0.8015016835016835, \"what's time the pool\": 0.784, \"what is pool\": 0.779959595959596, \"what time does the pool open?\": 0.696551724137931, \"how much to access pool?\": 0.639248366013072, \"do you have a pool\": 0.5766666666666667, \"do you have a pool?\": 0.5649707602339181}\n"
     ]
    }
   ],
   "source": [
    "req = json.loads(REQUEST)\n",
    "args = req['body']\n",
    "\n",
    "if 'text' not in args:\n",
    "    print(json.dumps({'text': None}))\n",
    "else:\n",
    "    new_question = pd.Series(str(args[\"text\"])).str.lower().str.strip()\n",
    "    \n",
    "    new_question_strip = [elem.strip().lower().split(' ') for elem in new_question]\n",
    "    flat_new_question = [ item for elem in new_question_strip for item in elem]\n",
    "    \n",
    "    #Find the question in the db if it exists\n",
    "    boolean_findings = questions_faq.str.contains(new_question[0], regex = False)\n",
    "\n",
    "    total_occurence = boolean_findings.sum()# Returns count of all boolean true.\n",
    "    \n",
    "    if(total_occurence > 0):\n",
    "        print(json.dumps({'Value of Similar questions':\"Question Already Exists\", \"Total Ocurrance\": str(total_occurence)}))\n",
    "    else:\n",
    "        list_filtered_questions = []\n",
    "        for base_service in standarized_base_services:\n",
    "            if base_service in flat_new_question:\n",
    "                print(base_service)\n",
    "            #continue #for question in questions_faq\n",
    "                \n",
    "                for question in questions_faq:\n",
    "                    if base_service in question:\n",
    "                        #print(\"true: base service exists in the question\")\n",
    "                        list_filtered_questions.append(question)\n",
    "                df_similarity_filtered=similarJaroWinkler(new_question[0],list_filtered_questions)\n",
    "                dict_similarity = dict(zip(df_similarity_filtered['questions'], df_similarity_filtered['similarity scores']))\n",
    "                sort_orders={k: v for k, v in sorted(dict_similarity.items(), key=lambda item: item[1], reverse = True)}\n",
    "                print(json.dumps(sort_orders))\n",
    "    \n",
    "            \n",
    "            if base_service not in standarized_base_services:\n",
    "#             print(\"another blah\")\n",
    "#             df_similarity_filtered=similarJaroWinkler(new_question[0],questions_faq)\n",
    "#             dict_similarity = dict(zip(df_similarity_filtered['questions'], df_similarity_filtered['similarity scores']))\n",
    "#             sort_orders={k: v for k, v in sorted(dict_similarity.items(), key=lambda item: item[1], reverse = True)}\n",
    "#             print(json.dumps(sort_orders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['where is the pool',\n",
       " 'do you have a pool?',\n",
       " 'how much to access pool?',\n",
       " 'when is the pool open',\n",
       " 'what is pool',\n",
       " 'do you have a pool',\n",
       " 'when is the pool open?',\n",
       " 'where is the pool?',\n",
       " \"what's time the pool\",\n",
       " 'what time does the pool close?',\n",
       " 'what time does the pool open?',\n",
       " \"where's the pool\",\n",
       " 'when does the pool open']"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_filtered_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_similarity_filtered\n",
    "# p = []\n",
    "# for i  in range(len(standarized_base_services)):\n",
    "#     matching = [s for s in questions_faq if standarized_base_services[i] in s]\n",
    "#     p.append(matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# req = json.loads(REQUEST)\n",
    "# args = req['body']\n",
    "\n",
    "# if 'text' not in args:\n",
    "#     print(json.dumps({'text': None}))\n",
    "# else:\n",
    "#     new_question = pd.Series(str(args[\"text\"])).str.lower().str.strip()\n",
    "    \n",
    "#     #Make comparision if word in Base service exists in the sentence, then only make comparision to those sentence that contains that service\n",
    "    \n",
    "#     #Find the question in the db if it exists\n",
    "#     boolean_findings = questions_faq.str.contains(new_question[0], regex = False)\n",
    "\n",
    "#     total_occurence = boolean_findings.sum()# Returns count of all boolean true.\n",
    "    \n",
    "#     if(total_occurence > 0):\n",
    "#         print(json.dumps({'Value of Similar questions':\"Question Already Exists\", \"Total Ocurrance\": str(total_occurence)}))\n",
    "\n",
    "#         :\n",
    "#         #show similarity scores for each sentence\n",
    "#         similar_questions_asked = [x for x in questions_faq for y in new_question if similar(x, y) > thresholdSequenceMatcher]\n",
    "#         print(json.dumps({'Value of Similar questions': similar_questions_asked, \"Total Ocurrance\": str(total_occurence)}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C:\\Users\\gabri\\Chulo Python>jupyter kernelgateway --KernelGatewayApp.api='kernel_gateway.notebook_http' --KernelGatewayApp.seed_uri='./SimilarityApi.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jupyter kernelgateway --KernelGatewayApp.api='kernel_gateway.notebook_http' --KernelGatewayApp.seed_uri='./SimilarityApi.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
