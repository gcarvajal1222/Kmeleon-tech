{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import warnings \n",
    "import nltk\n",
    "#from gensim.models import Word2Vec \n",
    "#from gensim.models.wrappers import FastText \n",
    "\n",
    "#from gensim.models import FastText\n",
    "\n",
    "\n",
    "\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import warnings \n",
    "import nltk \n",
    "import pandas as pd\n",
    "from pandas import *\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer() \n",
    "import re\n",
    "from sklearn.feature_extraction import text\n",
    "from builtins import input\n",
    "from statistics import mean\n",
    "from nltk import  pos_tag_sents\n",
    "\n",
    "\n",
    "from statistics import mode, StatisticsError\n",
    "\n",
    "import functools\n",
    "\n",
    "\n",
    "import re\n",
    "from sklearn.feature_extraction import text\n",
    "global cases\n",
    "\n",
    "from pandas import ExcelWriter\n",
    "import xlsxwriter\n",
    "\n",
    "import xlwt\n",
    "from xlwt.Workbook import *\n",
    "\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from statistics import *\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import seaborn as sns\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"faq-extract-text.xlsx\", sheet_name =\"only text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>faq text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How much to access gym?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How much to access pool?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>so you have wifi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>do you have wifi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is there wifi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>what is the price of wifi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>what is the price of wifi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>what is the price of wifi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>what is the price of wifi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>Where is the pool?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>241 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      faq text\n",
       "0      How much to access gym?\n",
       "1     How much to access pool?\n",
       "2             so you have wifi\n",
       "3             do you have wifi\n",
       "4                is there wifi\n",
       "..                         ...\n",
       "236  what is the price of wifi\n",
       "237  what is the price of wifi\n",
       "238  what is the price of wifi\n",
       "239  what is the price of wifi\n",
       "240         Where is the pool?\n",
       "\n",
       "[241 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['POS'] = pos_tag_sents( df['faq text'].apply(word_tokenize).tolist() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [(How, WRB), (much, JJ), (to, TO), (access, NN...\n",
       "1      [(How, WRB), (much, JJ), (to, TO), (access, NN...\n",
       "2       [(so, RB), (you, PRP), (have, VBP), (wifi, NNS)]\n",
       "3       [(do, VBP), (you, PRP), (have, VB), (wifi, NNS)]\n",
       "4                   [(is, VBZ), (there, EX), (wifi, JJ)]\n",
       "                             ...                        \n",
       "236    [(what, WP), (is, VBZ), (the, DT), (price, NN)...\n",
       "237    [(what, WP), (is, VBZ), (the, DT), (price, NN)...\n",
       "238    [(what, WP), (is, VBZ), (the, DT), (price, NN)...\n",
       "239    [(what, WP), (is, VBZ), (the, DT), (price, NN)...\n",
       "240    [(Where, WRB), (is, VBZ), (the, DT), (pool, NN...\n",
       "Name: POS, Length: 241, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['POS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # function to test if something is a noun\n",
    "# is_noun = lambda pos: pos[:2] == 'VB'\n",
    "# # do the nlp stuff\n",
    "\n",
    "# Finals_noun_list = []\n",
    "# for i in range(len(df['faq text'])):\n",
    "#     tokenized = nltk.word_tokenize(df['faq text'][i])\n",
    "#     nouns = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)]\n",
    "#     Finals_noun_list.append(nouns.vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    'Where is the pool',  # doc_id 0\n",
    "                                    # doc_id 1\n",
    "    'What time does the pool open',  # doc_id 2\n",
    "    'why is the pool closed',  # doc_id 3\n",
    "    'How to get to the pool',  # doc_id 4\n",
    "    'Is there a pool',  # doc_id 5\n",
    "    'When does the pool open'  # doc_id 6\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_Series=pd.Series(df['faq text'].str.lower().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    how much to access gym?\n",
       "1                   how much to access pool?\n",
       "2                           so you have wifi\n",
       "3                           do you have wifi\n",
       "4                              is there wifi\n",
       "                       ...                  \n",
       "89                      what's time the pool\n",
       "90                        where is the wifi?\n",
       "91                          where's the pool\n",
       "92    whats the cost of the lease break fee?\n",
       "93           i have william visiting tonight\n",
       "Length: 94, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document_Series1 = document_Series.apply(lambda x: list(nlp(x)))\n",
    "\n",
    "sample = document_Series.apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[how much to access gym?,\n",
       " how much to access pool?,\n",
       " so you have wifi,\n",
       " do you have wifi,\n",
       " is there wifi,\n",
       " how much does the buffet cost,\n",
       " hi,\n",
       " do yo have wifi,\n",
       " what is the price of wifi,\n",
       " where is wifi,\n",
       " where can i get wifi,\n",
       " do you have random stuff,\n",
       " when is the gym open,\n",
       " do you have a pool,\n",
       " is there a moon,\n",
       " do you have wifi?,\n",
       " do you have a gym,\n",
       " what is the price of visting the gym,\n",
       " what is the price of visiting the gym,\n",
       " where is the gym?,\n",
       " when is the pool open,\n",
       " where is the pool,\n",
       " what is pool,\n",
       " in which floor is the gym?,\n",
       " in which floor is the bar?,\n",
       " in which floor is the restaurant?,\n",
       " how much is wifi,\n",
       " do you have a pool?,\n",
       " do you have the moon,\n",
       " do you have a bar,\n",
       " what time does the pool open?,\n",
       " when is the pool open?,\n",
       " where is the tennis court?,\n",
       " where is the store?,\n",
       " hio,\n",
       " do you have a restaurant,\n",
       " waht si the price of the restaurant,\n",
       " what is the price of food,\n",
       " do you have food,\n",
       " what is the price of foodwhat is teh price of food,\n",
       " what is the price of foodwhat is the price of food,\n",
       " when can i get wifi,\n",
       " where is the wifi,\n",
       " whe ncan i get wifi,\n",
       " random stuff that isn't a real question,\n",
       " do you have parking,\n",
       " do you have a kitchen,\n",
       " when is the kitchen open,\n",
       " do yo have food,\n",
       " faq,\n",
       " do you have water,\n",
       " when can i get water?,\n",
       " do you sell beer,\n",
       " test,\n",
       " where is the moon,\n",
       " where is the gym located?,\n",
       " random question,\n",
       " how much does the gym cost?,\n",
       " do you have lego,\n",
       " what is the price of food?,\n",
       " waht si the price of wifi,\n",
       " what time is the gym open?,\n",
       " what time does the does open?,\n",
       " do you have food?,\n",
       " when does the gym close?,\n",
       " do you have a gym?,\n",
       " cancell,\n",
       " what time does the pool close?,\n",
       " where is the bar?,\n",
       " what can you do for me?,\n",
       " do you heve wifi,\n",
       " what is the  price of wifi,\n",
       " what time is the movie room open,\n",
       " hi oli,\n",
       " where is the pool?,\n",
       " what is the cost of breaking the lease fee?,\n",
       " what's the cost of breaking the lease fee?,\n",
       " there is a leak in my bathroom,\n",
       " what time is the movie room open?,\n",
       " how much are the lease break fees?,\n",
       " what's cost the lease break fee?,\n",
       " what's cost the lease break fee,\n",
       " what's the cost of the lease break fee?,\n",
       " when does the pool open,\n",
       " random text,\n",
       " what time does the office close?,\n",
       " is there a dog walking service in the residence?,\n",
       " what time does the tennis court open?,\n",
       " what time does the bar open?,\n",
       " what's time the pool,\n",
       " where is the wifi?,\n",
       " where's the pool,\n",
       " whats the cost of the lease break fee?,\n",
       " i have william visiting tonight]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [tok for tok in sample]\n",
    "list(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmas = []\n",
    "# for tok in tokens:\n",
    "#     if tok.pos_ in (\"VERB\", \"ADJ\", \"ADV\"):\n",
    "#         lemmas.append((tok.lemma_.lower(), tok.tag_, tok.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_Series=pd.Series(df['faq text'].str.lower().unique()) #extract unique questions\n",
    "\n",
    "\n",
    "\n",
    "faqs = document_Series.apply(nlp) # scapcy nlp\n",
    "\n",
    "\n",
    "word_faq = []\n",
    "pos = []   \n",
    "for i in range(len(faqs)):\n",
    "    pos.append(\"new faq\")\n",
    "    word_faq.append(\"new faq\")\n",
    "    for word in faqs[i]:\n",
    "        if word.pos_ in (\"NOUN\"):\n",
    "            pos.append(word.pos_)\n",
    "            word_faq.append(word.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_faq</th>\n",
       "      <th>word_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>new faq</td>\n",
       "      <td>new faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>access</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gym</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>new faq</td>\n",
       "      <td>new faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pool</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>lease</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>break</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>fee</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>new faq</td>\n",
       "      <td>new faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>tonight</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>223 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    word_faq word_pos\n",
       "0    new faq  new faq\n",
       "1     access     NOUN\n",
       "2        gym     NOUN\n",
       "3    new faq  new faq\n",
       "4       pool     NOUN\n",
       "..       ...      ...\n",
       "218    lease     NOUN\n",
       "219    break     NOUN\n",
       "220      fee     NOUN\n",
       "221  new faq  new faq\n",
       "222  tonight     NOUN\n",
       "\n",
       "[223 rows x 2 columns]"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tags = pd.DataFrame({'word_faq': word_faq,\n",
    "     'word_pos': pos\n",
    "    })\n",
    "df_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_faq = len(word_faq) \n",
    "idx_list_faq = [idx + 1 for idx, val in\n",
    "            enumerate(word_faq) if val == \"new faq\"] \n",
    "\n",
    "res_faq = [word_faq[i: j] for i, j in\n",
    "        zip([0] + idx_list, idx_list + \n",
    "        ([size] if idx_list[-1] != size else []))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_pos = len(pos) \n",
    "idx_list_pos = [idx + 1 for idx, val in\n",
    "            enumerate(pos) if val == \"new faq\"] \n",
    "\n",
    "res_pos = [pos[i: j] for i, j in\n",
    "        zip([0] + idx_list, idx_list + \n",
    "        ([size] if idx_list[-1] != size else []))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_final_list = []\n",
    "for i in range(len(res_faq)):\n",
    "    temp_dict = dict(zip(res_pos[i], res_faq[i]))\n",
    "    temp_final_list.append(temp_dict)\n",
    "    #final_dict.update(map(temp_dict.keys(), temp_dict.values()))\n",
    "\n",
    "#getting rid off new faq dicts\n",
    "for i in range(len(temp_final_list)):\n",
    "    temp_final_list[i].pop(\"new faq\", None)\n",
    "\n",
    "#gettting rid off blank dicts\n",
    "Final_dict=[i for i in temp_final_list if i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'NOUN': 'pool'},\n",
       " {'NOUN': 'wifi'},\n",
       " {'NOUN': 'stuff'},\n",
       " {'NOUN': 'moon'},\n",
       " {'NOUN': 'gym'},\n",
       " {'NOUN': 'gym'},\n",
       " {'NOUN': 'pool'},\n",
       " {'NOUN': 'pool'},\n",
       " {'NOUN': 'floor'},\n",
       " {'NOUN': 'pool'},\n",
       " {'NOUN': 'time'},\n",
       " {'NOUN': 'court'},\n",
       " {'NOUN': 'restaurant'},\n",
       " {'NOUN': 'food'},\n",
       " {'NOUN': 'foodwhat'},\n",
       " {'NOUN': 'price'},\n",
       " {'NOUN': 'wifi'},\n",
       " {'NOUN': 'kitchen'},\n",
       " {'NOUN': 'water'},\n",
       " {'NOUN': 'gym'},\n",
       " {'NOUN': 'price'},\n",
       " {'NOUN': 'wifi'},\n",
       " {'NOUN': 'food'},\n",
       " {'NOUN': 'gym'},\n",
       " {'NOUN': 'pool'},\n",
       " {'NOUN': 'price'},\n",
       " {'NOUN': 'room'},\n",
       " {'NOUN': 'fee'},\n",
       " {'NOUN': 'time'},\n",
       " {'NOUN': 'lease'},\n",
       " {'NOUN': 'break'},\n",
       " {'NOUN': 'fee'},\n",
       " {'NOUN': 'office'},\n",
       " {'NOUN': 'time'},\n",
       " {'NOUN': 'pool'},\n",
       " {'NOUN': 'lease'},\n",
       " {'NOUN': 'tonight'}]"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_list = []\n",
    "for i in range(len(Final_dict)):\n",
    "    nouns_dict={ key: value for key, value in Final_dict[i].items() if key == \"NOUN\" }\n",
    "    nouns_list.append(nouns_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_nouns = df_tags[(df_tags['word_pos'].isin(['NOUN', 'new faq']))]\n",
    "# values, counts = np.unique(df_nouns['word_faq'], return_counts=True)\n",
    "\n",
    "# print(values, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['where', 'is', 'the', 'pool'], ['what', 'time', 'does', 'the', 'pool', 'open'], ['why', 'is', 'the', 'pool', 'closed'], ['how', 'to', 'get', 'to', 'the', 'pool'], ['is', 'there', 'a', 'pool'], ['when', 'does', 'the', 'pool', 'open']]\n",
      "[(0, 1.0), (1, 1.0), (2, 1.0), (3, 1.0), (4, 1.0), (5, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "from collections import defaultdict\n",
    "\n",
    "documents1 = [\n",
    "    'Where is the pool',  \n",
    "    'What time does the pool open',\n",
    "    'why is the pool closed',  \n",
    "    'How to get to the pool',  \n",
    "    'Is there a pool',  \n",
    "    'When does the pool open'  \n",
    "]\n",
    "\n",
    "stoplist = set([])\n",
    "\n",
    "texts = [[word.lower() for word in document.split()\n",
    "          if word.lower() not in stoplist]\n",
    "         for document in documents1]\n",
    "\n",
    "print(texts)\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "texts = [[token for token in text if frequency[token] > 1]\n",
    "         for text in texts]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# doc2bow counts the number of occurences of each distinct word,\n",
    "# converts the word to its integer word id and returns the result\n",
    "# as a sparse vector\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=1)\n",
    "doc = \"where is the pool located\"\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "\n",
    "# convert the query to LSI space\n",
    "vec_lsi = lsi[vec_bow]\n",
    "index = similarities.MatrixSimilarity(lsi[corpus])\n",
    "\n",
    "# perform a similarity query against the corpus\n",
    "sims = index[vec_lsi]\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "\n",
    "print (sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1.0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words ['access', 'bar', 'bathroom', 'beer', 'break', 'breaking', 'buffet', 'cancell', 'close', 'cost', 'court', 'does', 'dog', 'faq', 'fee', 'fees', 'floor', 'food', 'foodwhat', 'gym', 'heve', 'hi', 'hio', 'isn', 'kitchen', 'leak', 'lease', 'lego', 'located', 'moon', 'movie', 'ncan', 'office', 'oli', 'open', 'parking', 'pool', 'price', 'question', 'random', 'real', 'residence', 'restaurant', 'room', 'sell', 'service', 'si', 'store', 'stuff', 'teh', 'tennis', 'test', 'text', 'time', 'tonight', 'visiting', 'visting', 'waht', 'walking', 'water', 'whats', 'whe', 'wifi', 'william', 'yo']\n",
      "centers: [[0.01630002 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.04428478 0.\n",
      "  0.95876749 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.01532941\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.04118227 0.01767767 0.00883883 0.05080151 0.01518885\n",
      "  0.00898453 0.0125     0.01481376 0.05767283 0.02426878 0.05573519\n",
      "  0.00625    0.0375     0.05503932 0.0087377  0.017873   0.04459522\n",
      "  0.         0.         0.01168742 0.132105   0.0125     0.00593307\n",
      "  0.02311368 0.01767767 0.05995388 0.0125     0.         0.0375\n",
      "  0.04168281 0.00853708 0.00763851 0.01028441 0.06468972 0.0125\n",
      "  0.01344107 0.00383799 0.01478502 0.02982328 0.00593307 0.00625\n",
      "  0.02820712 0.04168281 0.00883883 0.00625    0.00686828 0.0125\n",
      "  0.01478502 0.         0.02426878 0.0125     0.00956234 0.06825162\n",
      "  0.00773076 0.00605975 0.         0.00686828 0.00625    0.075\n",
      "  0.007185   0.00853708 0.01239229 0.00773076 0.02186549]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.         0.         0.        ]\n",
      " [0.09654931 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.02714663 0.04678459 0.         0.06904081\n",
      "  0.         0.         0.         0.         0.03260397 0.\n",
      "  0.         0.64460328 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.03368731 0.\n",
      "  0.         0.         0.         0.         0.18870056 0.\n",
      "  0.         0.10306361 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.02432562\n",
      "  0.         0.13995989 0.03093737 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.26414933\n",
      "  0.03790561 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.69505253 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.04218152 0.\n",
      "  0.         0.01850633 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.04218152 0.         0.\n",
      "  0.         0.         0.36418382 0.         0.        ]]\n",
      "labels [3 0 2 2 2 1 2 1 1 2 4 2 2 4 4 2 2 1 3 0 1 2 3 3 3 3 3 3 3 3 3 3 3 3 2 0 0\n",
      " 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3 1 1 2 2 0 2 0 0 1 1 2 2 2 2 1 0 1 1\n",
      " 1 1 2 1 1 1 2 1 1 1 4 4 1 1 4 4 4 4 4 4 4 4 4 4 2 4 2 2 2 2 2 1 2 2 4 1 2\n",
      " 2 2 2 2 2 2 3 1 1 1 1 2 1 1 1 1 2 1 1 1 2 1 2 0 1 1 2 1 3 1 1 3 3 1 1 4 4\n",
      " 3 1 4 4 2 1 4 2 1 3 3 3 3 1 0 1 1 1 2 1 2 0 0 2 1 1 4 0 1 0 1 0 1 1 1 1 1\n",
      " 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 1 1 2 1 0 0 0 0 0 0 0 0 1 1 1 4 4 0 0 0 2\n",
      " 2 0 0 0 1 1 1 0 1 1 0 1 1 4 4 4 4 4 0]\n",
      "intertia: 100.25481013833773\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Top words per cluster:\n",
      "Cluster: 0 texts: 55\n",
      "\tpool\n",
      "\topen\n",
      "\taccess\n",
      "\ttime\n",
      "\thio\n",
      "\tfoodwhat\n",
      "\tgym\n",
      "\theve\n",
      "\thi\n",
      "\tkitchen\n",
      "Cluster: 1 texts: 80\n",
      "\thi\n",
      "\twater\n",
      "\ttime\n",
      "\topen\n",
      "\tlease\n",
      "\tcost\n",
      "\tdoes\n",
      "\tfee\n",
      "\tbreak\n",
      "\tfood\n",
      "Cluster: 2 texts: 50\n",
      "\twifi\n",
      "\tyo\n",
      "\tisn\n",
      "\tfood\n",
      "\tfoodwhat\n",
      "\tgym\n",
      "\theve\n",
      "\thi\n",
      "\thio\n",
      "\tkitchen\n",
      "Cluster: 3 texts: 26\n",
      "\tgym\n",
      "\topen\n",
      "\tvisiting\n",
      "\tprice\n",
      "\taccess\n",
      "\tdoes\n",
      "\tcost\n",
      "\tlocated\n",
      "\tfloor\n",
      "\tvisting\n",
      "Cluster: 4 texts: 30\n",
      "\tprice\n",
      "\twifi\n",
      "\tfood\n",
      "\tsi\n",
      "\twaht\n",
      "\tfoodwhat\n",
      "\tteh\n",
      "\thi\n",
      "\tfloor\n",
      "\tgym\n",
      "\n",
      "\n",
      "Prediction\n",
      "What time is the pool closed\n",
      "Cluster: 0 texts: 56\n",
      "\tpool\n",
      "\topen\n",
      "\taccess\n",
      "\ttime\n",
      "\thio\n",
      "\tfoodwhat\n",
      "\tgym\n",
      "\theve\n",
      "\thi\n",
      "\tkitchen\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.metrics import adjusted_rand_score\n",
    "# import numpy\n",
    "\n",
    "# texts = df['faq text']\n",
    "\n",
    "# # vectorization of the texts\n",
    "# vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "# X = vectorizer.fit_transform(texts)\n",
    "# # used words (axis in our multi-dimensional space)\n",
    "# words = vectorizer.get_feature_names()\n",
    "# print(\"words\", words)\n",
    "\n",
    "# n_clusters=5\n",
    "# number_of_seeds_to_try=10\n",
    "# max_iter = 300\n",
    "# number_of_process=2 # seads are distributed\n",
    "# model = KMeans(n_clusters=n_clusters, max_iter=max_iter, n_init=number_of_seeds_to_try, n_jobs=number_of_process).fit(X)\n",
    "\n",
    "# labels = model.labels_\n",
    "# # indices of preferible words in each cluster\n",
    "# ordered_words = model.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "# print(\"centers:\", model.cluster_centers_)\n",
    "# print(\"labels\", labels)\n",
    "# print(\"intertia:\", model.inertia_)\n",
    "\n",
    "# texts_per_cluster = numpy.zeros(n_clusters)\n",
    "# for i_cluster in range(n_clusters):\n",
    "#     for label in labels:\n",
    "#         if label==i_cluster:\n",
    "#             texts_per_cluster[i_cluster] +=1\n",
    "\n",
    "#             print(\"Top words per cluster:\")\n",
    "# for i_cluster in range(n_clusters):\n",
    "#     print(\"Cluster:\", i_cluster, \"texts:\", int(texts_per_cluster[i_cluster])),\n",
    "#     for term in ordered_words[i_cluster, :10]:\n",
    "#         print(\"\\t\"+words[term])\n",
    "\n",
    "# print(\"\\n\")\n",
    "# print(\"Prediction\")\n",
    "\n",
    "# text_to_predict = \"What time is the pool closed\"\n",
    "# Y = vectorizer.transform([text_to_predict])\n",
    "# predicted_cluster = model.predict(Y)[0]\n",
    "# texts_per_cluster[predicted_cluster]+=1\n",
    "\n",
    "# print(text_to_predict)\n",
    "# print(\"Cluster:\", predicted_cluster, \"texts:\", int(texts_per_cluster[predicted_cluster])),\n",
    "# for term in ordered_words[predicted_cluster, :10]:\n",
    "#     print(\"\\t\"+words[term])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where is the pool',\n",
       " 'What time does the pool open',\n",
       " 'why is the pool closed',\n",
       " 'How to get to the pool',\n",
       " 'Is there a pool',\n",
       " 'When does the pool open']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_faq = pd.read_excel(\"faq-extract-text.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AskForPrice', 'AskForExistence', 'AskForPlace', 'AskForTimes',\n",
       "       'None'], dtype=object)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_faq['Intent'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('StandarizedFaq.xlsx') as writer:\n",
    "\n",
    "        document_Series.to_excel(writer, sheet_name = 'Standarized Faq', header = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Fuzzy matching for similar words that IRI knows (in the brand name) and getting rid off them\n",
    "\n",
    "import difflib\n",
    "    \n",
    "def similar(a, b):\n",
    "    return difflib.SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "    \n",
    "\n",
    "x1_none = (df_None['word'].str.strip()) #words that do not conatin tag\n",
    "x1_contains_tag = (df_contains_tag['word'].str.strip()) #words that contain tag\n",
    "y1_words_in_brand = (C_UW_B['Word'].str.strip()) #words in v4_brand field \n",
    "threshold = 0.90  #Can test with different ratios for different categories \n",
    "\n",
    "    \n",
    "\n",
    "result_fuzzy_words_df_none = [x for x in x1_none for y in y1_words_in_brand if similar(x, y) > threshold]\n",
    "\n",
    "print(\"counts of fuzzywords in df_none :\", len(result_fuzzy_words_df_none)) \n",
    "\n",
    "    \n",
    "\n",
    "result_fuzzy_words_df_contains_tag = [x for x in x1_contains_tag for y in y1_words_in_brand if similar(x, y) > threshold]\n",
    "\n",
    "print(\"counts of fuzzywords in df_contains_tag:\", len(result_fuzzy_words_df_contains_tag)) \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "#getting rid off fuzzy matches that are in high ratios\n",
    "\n",
    "    # Example: nestl and nestle, hersheys and hersheeys for chocolate_candy category \n",
    "\n",
    "    \n",
    "\n",
    "#Converting list to pd.series to only get the unique fuzzy words\n",
    "\n",
    "result_fuzzy_words_df_none_S= pd.Series(result_fuzzy_words_df_none).unique()\n",
    "\n",
    "result_fuzzy_words_df_contains_tag_S =pd.Series(result_fuzzy_words_df_contains_tag).unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
